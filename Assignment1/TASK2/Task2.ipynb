{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2 - Image stitching\n",
    "\n",
    "Group Number: 14\n",
    "\n",
    "### 1. Data import and SIFT Extraction\n",
    "\n",
    "Complete ***get_panorama_data(..)*** in _dataset.py_ to read the images and extract the SIFT keypoints and descriptors per image. Check your implementation by plotting the result using ***utils.plot_keypoints(..)***.\n",
    "\n",
    "\n",
    "***Submission:*** Save the *second* image (from left) of the campus dataset as **task2_keypoints.png** using ***utils.plot_keypoints(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport transforms, panorama, mapping, dataset, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transforms\n",
    "import panorama\n",
    "import mapping\n",
    "import dataset\n",
    "import utils\n",
    "import cv2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "data_path = 'data/office_rot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student_code start\n",
    "images, keypoints, descriptors = dataset.get_panorama_data(data_path)\n",
    "\n",
    "second_img_index = 1\n",
    "utils.plot_keypoints(images[second_img_index],\n",
    "                     keypoints[second_img_index],\n",
    "                     'Group 14',\n",
    "                     'task2_keypoints.png')\n",
    "#student_code end\n",
    "\n",
    "# check import\n",
    "print('Number of images: ',len(images))\n",
    "print('Keypoints length: ', len(keypoints[0]))\n",
    "print('Descriptor shape: ',descriptors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### What is the meaning of the size of the drawn circles and lines inside the circles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The **circles** represent the positions of the keypoints.\n",
    "\n",
    "The **size of each circle** corresponds to the scale of the keypoint. Bigger circles mark features that appear on larger patterns or structures in the image.\n",
    "\n",
    "The **line inside each keypoint circle** represents its orientation, which corresponds to the main gradient direction around the point. This allows the keypoint descriptor to remain consistent even if the image is rotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Matching\n",
    "\n",
    "Let's take a look at the matching. Plot the matches between two adjacent images using ***mapping.calculate_matches(..)*** and ***utils.plot_matches(..)***.\n",
    "\n",
    "\n",
    "***Submission:*** Save plot of matches between the *second and the third image* image of the campus dataset as **task2_matches.png** using ***utils.plot_matches(..)***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index1 = 1\n",
    "index2 = 2\n",
    "\n",
    "#student_code start\n",
    "matches = mapping.calculate_matches(descriptors[index1], descriptors[index2])\n",
    "\n",
    "utils.plot_matches(images[index1], images[index2], \n",
    "                   keypoints[index1], keypoints[index2], \n",
    "                   matches,\n",
    "                   'Group 14',\n",
    "                   'task2_matches.png')\n",
    "#student_code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Describe below how the matching, based on LOWE, works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The LOWE matching algorithmus is used to match local SIFT descriptors between two overlapping images of the same scene. The goal is to identify keypoints in image 1 that correspond to the same physical points in image 2.\n",
    "\n",
    "1. For each detected keypoint in both images, a SIFT descriptor is computed. A descriptor is a 128 dimensional vector describing the local gradient structure around the keypoint.\n",
    "\n",
    "2. To find potential correspondences, each descriptor from image 1 is compared against all descriptors in image 2 using euclidean distance. In our case: For each descriptor, the two nearest neighbours (k=2) in the second image are retrieved (matches = bf.knnMatch(desk1, desk2, k=2)).\n",
    "\n",
    "3. Most false matches occur when different descriptors have very similar distances to a given descriptor. The LOWE ratio test solves the problem: A match is accepted only if the best match is significantly better than the second best. (m.distance / n.distance < 0.8). It removes the majority of incorrect correspondences becauce ambiguous matches are discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image Registration using RANSAC\n",
    "\n",
    "As you can see in the keypoint matching plot above, the matching algorithm still has some wrong connections. To remove those outliers, you will implement RANSAC and use the remaining inliers to estimate a final tranformation matrix (homography) between two given images. Implement RANSAC in ***get_transform(..)*** in _transforms.py_.\n",
    "\n",
    "Check your implementation by plotting the result using ***utils.plot_matches(..)***.\n",
    "\n",
    "\n",
    "***Submission:*** Save a plot showing the matches between the *second and the third image* of the campus dataset again, using the calculated inliers, as **task2_matches_ransac.png** using ***utils.plot_matches(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#student_code start\n",
    "trans, inliers = transforms.get_transform(keypoints[index1], keypoints[index2], matches)\n",
    "\n",
    "# Extract keypoints of inliers, because drawMatches needs correct inliers list as parameter\n",
    "kp1_inliers = [keypoints[index1][matches[i].queryIdx] for i in inliers]\n",
    "kp2_inliers = [keypoints[index2][matches[i].trainIdx] for i in inliers]\n",
    "\n",
    "# Create new DMatch because indices is a reduced list of inliers\n",
    "# In other words: Outliers are removed\n",
    "inlier_matches = [cv2.DMatch(idx, idx, 0) for idx in range(len(inliers))] \n",
    "\n",
    "utils.plot_matches(images[index1], images[index2], \n",
    "                   kp1_inliers, kp2_inliers, \n",
    "                   inlier_matches,\n",
    "                   'Group 14',\n",
    "                   'task2_matches_ransac.png')\n",
    "#student_code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### What is the difference to the set of all putative matches you plotted before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The set of putative matches conains all descriptor based correspondences including many outliers.\n",
    "\n",
    "After applying RANSAC, only those matches remain that are geometrically consistent with a homography between two selected images. Thus, RANSAC removes the false matches.The RANSAC inliers is the subset of putative matches that satisfy the geometric constraint (homography).\n",
    "\n",
    "Visually, the matches after RANSAC appear almost horizontally alligned. In contrast, the plot of all putative matches contains many criss-corssing and noisy connection lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Align Images\n",
    "\n",
    "Test your homography after RANSAC by transforming a chosen image to the corresponding image on the right and plot with ***transforms.plot_transformed_image(..)***.\n",
    "\n",
    "_***HINT:***_\n",
    "_cv2.warpPerspective(..)_\n",
    "\n",
    "\n",
    "***Submission:*** Save the second image of the campus dataset transformed onto the third one as **task2_matches_transformed.png** using ***utils.plot_transformed_image(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#student_code start\n",
    "height, width, channel = images[index1].shape\n",
    "transformed_image1 = cv2.warpPerspective(images[index1], trans, (width, height)) # transform second onto third image with H\n",
    "utils.plot_transformed_image(transformed_image1, \n",
    "                             images[index2],\n",
    "                             'Group 14', \n",
    "                             'task2_matches_transformed.png')\n",
    "#student_code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image Stitching\n",
    "\n",
    "Before, you implemented the basics to calculated homographies between two images. Further, transform all images to a reference image, usually the center one, to get a balanced panorama. Implement and use the method ***to_center(..)*** in _transforms.py_ to get all homographies to the center image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student_code start\n",
    "\n",
    "#student_code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panorama Extents\n",
    "\n",
    "One final step before panorama composition is to estimate the final panorama size based on the obtained homographies. Implement ***transforms.get_panorama_extents(..)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student_code start\n",
    "\n",
    "\n",
    "\n",
    "#student_code end\n",
    "\n",
    "print(\"Panorama dimension: \", height, \" \",width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Panorama  \n",
    "\n",
    "Here is where the magic happens. Using the homographies, translation matrix and panorama extents, you can now stitch the images to a panorama. Implement ***panorama.get_simple(..)*** and check your result with ***utils.plot_panorama(..)***.\n",
    "\n",
    "***Submission:*** Save the campus panorama as **task2_panorama_simple.png** using ***utils.plot_panorama(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student_code start\n",
    "\n",
    "#student_code end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Panorama Feathering\n",
    "\n",
    "The simple panorama might look geometrically correct, but not pleasant enough yet. The intensities are simply stacked. Implement a color blending method to improve the panorama output. \n",
    "\n",
    "Complete ***panorama.get_blended(..)*** and check your result with ***utils.plot_panorama(..)***.\n",
    "\n",
    "***Submission:*** Save the improved blended campus panorama as **task2_panorama_blended.png** using ***utils.plot_panorama(..)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#student_code start\n",
    "\n",
    "#student_code end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Compare the result achieved with feathering to the result where no blending has been performed. What is the difference of the two results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Examine if the presented scheme of SIFT interest point detection and RANSAC-based alignment is invariant to changes in image rotation and scale. Thus, resize and rotate the second image of the campus dataset and repeat the panorama process. What do you observe?\n",
    "\n",
    "***Submission:*** Save the resulting panorama with the rotated images as file ***panorama_rotated_blended.png*** (feathered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Once again, repeat the panorama procedure with your own image sequence. Save and discuss the achieved results. The result might look quite realistic at a first glance but can you spot any errors by looking on details?\n",
    "\n",
    "***Submission:*** Save the resulting panorama as file ***panorama_own.png*** (feathered) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
